{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_trees.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFUrqgFvI-Kq"
      },
      "source": [
        "My very first example of understanding and learning Decision trees. And an  attempt to code the logic for this simple example in python.\n",
        "\n",
        "https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1\n",
        "\n",
        "https://www.kaggle.com/pragyanbo/decision-trees-from-scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf6dNwRRfaXK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN6_g3FTDft7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wm32eluJkrV"
      },
      "source": [
        "###1. Creating the Demo dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk1PbzzrBwUJ",
        "outputId": "25caecfe-e732-43f2-ad98-7ab30f427909"
      },
      "source": [
        "# The initial dataset \n",
        "arr = np.array([['sunny','sunny','overcast','rainy','rainy','rainy','overcast','sunny','sunny','rainy','sunny','overcast',\n",
        "                  'overcast','rainy'],\n",
        "                ['hot','hot','hot','mild','cool','cool','cool','mild','cool','mild','mild','mild','hot','mild'],\n",
        "                ['high','high','high','high','normal','normal','normal','high','normal','normal','normal','high',\n",
        "                 'normal','high'],\n",
        "                ['false','true','false','false','false','true','true','false','false','false','true','true','false','true'],\n",
        "                ['no','no','yes','yes','yes','no','yes','no','yes','yes','yes','yes','yes','no']]) \n",
        "\n",
        "# converting out 2D array into a dataframe with named columns \n",
        "df = pd.DataFrame(arr.T)\n",
        "df.columns = ['Outlook','Temp','Humidity','Windy','Play']\n",
        "\n",
        "print(df)\n",
        "\n",
        "# splitting the data into features(X) and target(y)\n",
        "df_x = df.iloc[:,:-1]\n",
        "y = pd.DataFrame(df.iloc[:,-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Outlook  Temp Humidity  Windy Play\n",
            "0      sunny   hot     high  false   no\n",
            "1      sunny   hot     high   true   no\n",
            "2   overcast   hot     high  false  yes\n",
            "3      rainy  mild     high  false  yes\n",
            "4      rainy  cool   normal  false  yes\n",
            "5      rainy  cool   normal   true   no\n",
            "6   overcast  cool   normal   true  yes\n",
            "7      sunny  mild     high  false   no\n",
            "8      sunny  cool   normal  false  yes\n",
            "9      rainy  mild   normal  false  yes\n",
            "10     sunny  mild   normal   true  yes\n",
            "11  overcast  mild     high   true  yes\n",
            "12  overcast   hot   normal  false  yes\n",
            "13     rainy  mild     high   true   no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGur2oFbJeXR"
      },
      "source": [
        "###2. Entropy calculation of the dataset. \n",
        "First task, when given a dataset is to calculate the entropy of the entire dataset.So, we write a function of entropy calculation.\n",
        "\n",
        "**Entropy**\\\n",
        "Entropy H(S) is a measure of the amount of uncertainity in the data set (S) (i.e entropy characterizes the (data)).\n",
        "\n",
        "\\begin{align}\n",
        " H(s) = \\sum_{c\\in C} -p(c) log_2 p(c)\n",
        "\\end{align}\n",
        "\n",
        "where, \\\n",
        "S - The current data set for which entropy is being calculated (changes in every iteration)\\\n",
        "C - Set of classes in S = {yes,no}\\\n",
        "p(c) - The proportion of the number of the number of elements in class c to the no elements in set S.\\\n",
        "When $H(S)=0$, the set $S$ is perfectly classified (i.e all elements in $S$ are of the same class).\\\n",
        "\n",
        "In $ID3$, entropy is calculated for each remaining attribute. The attribute with the smallest entropy is used to splot the set $S$ on their iteration. The higher the entropy, the higher the potential to improve the classification here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbAfiC12QXtm"
      },
      "source": [
        "###3. To select our first node \n",
        "\n",
        "To crete a tree we need to have a root node first and we know that nodes are features/attributes ( Outlook,Temp,Humidity & windy).\n",
        "\n",
        "Q. So which one do we pick first ?\\\n",
        "Determine the attirbute that best classifies the training data; use this attribute at the root of the tree. Repeat this process for each branch. This mean, we are performning top-down greedy search through the space of possible decision trees.\n",
        "\n",
        "Q. How do we choose the best attribute ? \\\n",
        "Use the attribute with the highest \"Information gain\" in ID3.\n",
        "\n",
        "**Information Gain**\n",
        "\n",
        "Information gain IG(A) is the measure of the difference in entropy from before to after the set $S$ in split on an attribute $A$. In other words. how much uncertainity in $S$ was reduced after splitting set $S$ on attribute $A$. \n",
        "\n",
        "\\begin{align}\n",
        "IG(A,S) = H(S) - \\sum_{t\\in T} p(t) H(t)\n",
        "\\end{align}\n",
        "\n",
        "where,\\\n",
        "$H(S)$ : Entropy of set $S$.\\\n",
        "$T$ : The subsets created from splitting set $S$ by attribute $A$ such that $S = \\cup_{t \\in T}t$\\\n",
        "$p(t)$ : The proportion of the number of elements in $t$ to the no of elements in set $S$.\n",
        "$H(t)$ : Entropy of subset $t$.\n",
        "\n",
        "In ID3, Information Gain can be calculated (instead of entropy) for each remaining attribute. The attribute with the **largest Information Gain** is used to split the set $S$ on this iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GejM-5Q5iGJy"
      },
      "source": [
        "'''First task, when given a dataset is to calculate the entropy of the entire dataset.So, we write a function of entropy calculation '''\n",
        "def entropy(df,y):\n",
        "  # Entropy of the entire dataset   \n",
        "  ''' comments to understand the implementation in first pass'''\n",
        "  # define the variable entropy to hold th value of the calculated entropy\n",
        "  entropy = 0\n",
        "  class_val = y.value_counts()\n",
        "  for i in range(len(class_val)):\n",
        "    entropy += - (np.divide(class_val[i],np.sum(class_val))) * math.log(np.divide(class_val[i],np.sum(class_val)),2)\n",
        "  return entropy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoGc7J9ITnw4"
      },
      "source": [
        "'''Updating the entropy function for each attribute w,r,t 'y' and then pass the result to information gain function'''\n",
        "\n",
        "def entropy_features(df,y,cols):\n",
        "  # Using the groupby function to group our feature vector subclasses with target class\n",
        "  # define the list entropy to hold the values of the calculated entropies of all the viable features\n",
        "  avg_entropy = []\n",
        "  avg_cal = 0\n",
        "\n",
        "  for k in range(0,df.shape[1]-1):\n",
        "  \n",
        "    unq_attr = df.iloc[:,k].unique()\n",
        "    f = df.groupby([cols[k],cols[-1]]).size().reset_index(name=\"counts\")\n",
        "    g = df.iloc[:,k].value_counts().reset_index(name=\"counts\")\n",
        "    avg_cal = 0\n",
        "    #print(\"We are here in enotrpy_features\")\n",
        "\n",
        "    for i in range(0,len(unq_attr)):\n",
        "      cal = 0 \n",
        "      for j in range(0,len(f)):\n",
        "\n",
        "        if f.iloc[j,0] == g.iloc[i,0]:\n",
        "          cal += - (np.divide(f.iloc[j,-1],g.iloc[i,1])) * math.log(np.divide(f.iloc[j,-1],g.iloc[i,-1]),2)\n",
        "      avg_cal += np.divide(g.iloc[i,-1],len(df)) * cal\n",
        "    avg_entropy.append(avg_cal)\n",
        "  return avg_entropy\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4R9hPCFcviK"
      },
      "source": [
        "def information_gain(entropy_l,entropy_d,cols):\n",
        "\n",
        "  # entropy_d : the entropy of the target column \n",
        "  # entropy_l : list of all average entropy of all the features considered for this subset of the dataset.\n",
        "  node= np.argmax(entropy_d - entropy_l)\n",
        "  return cols[node],node "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr3mHHuvWIpq"
      },
      "source": [
        "def sub_dataframe(df,y,cols,node,node_attr):\n",
        "  \n",
        "    sub_df = df[df.iloc[:,node]==node_attr]\n",
        "    x = sub_df.drop(columns=cols[node])\n",
        "    y = sub_df.iloc[:,-1]\n",
        "    return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvrAUdkamUYI"
      },
      "source": [
        "def build_tree(df,y,tree):\n",
        "\n",
        "  # Define the tree dictionary, to hold the tree structure, as we recursively traverse the tree.\n",
        "  cols = df.columns\n",
        " \n",
        "  # Number of unique target values \n",
        "  entropy_ds = entropy(df,y) \n",
        " \n",
        "  # get the avg entropy of all features of the df and get the best fit, using information gain function\n",
        "  entropy_l = entropy_features(df,y,cols)\n",
        "  feature, node = information_gain(entropy_l,entropy_ds,cols)\n",
        " \n",
        "  tree = {}\n",
        "  tree.update({feature:{}})\n",
        "\n",
        "  #get the number of unique attributes in the feature. \n",
        "  node_attr = df.iloc[:,node].unique()\n",
        "\n",
        "  \n",
        "  for key in range(0,len(node_attr)):\n",
        "\n",
        "    tree[feature].update({node_attr[key]:\"\"})\n",
        "    # get the sub_dataFrame w.r.t to the attribute selected \n",
        "    sub_df,y = sub_dataframe(df,y,cols,node,node_attr[key])\n",
        "    count = y.nunique()\n",
        "    \n",
        "    # check the counts with respect to target \n",
        "    if count == 1:\n",
        "      tree[feature][node_attr[key]] = y.iloc[0]\n",
        "    else: \n",
        "      tree[feature][node_attr[key]] = build_tree(sub_df,y,tree)\n",
        "  return tree\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45SZ8p1sn9K5",
        "outputId": "c778298e-e40d-4b1b-8c00-7de2a7f06412"
      },
      "source": [
        "tree = {}\n",
        "build_tree(df,y,tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Outlook': {'overcast': 'yes',\n",
              "  'rainy': {'Windy': {'false': 'yes', 'true': 'no'}},\n",
              "  'sunny': {'Humidity': {'high': 'no', 'normal': 'yes'}}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA6Q1WeZnQ94"
      },
      "source": [
        "To do :\n",
        "1. Try to update the build tree. to accomodate max depth. nd implement with another example. load_breast data \n",
        "2. With the help of pydot.. plot the tree. \n",
        "\n",
        "3. Regression tree (CART) : Gini Index "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbAziRzSTkeg",
        "outputId": "e18decd0-dd5a-472d-899e-cefe48d1a48d"
      },
      "source": [
        "cols = df.columns\n",
        "attr = df.iloc[:,0].unique()\n",
        "print(cols)\n",
        "print(attr)\n",
        "\n",
        "tree = {}\n",
        "tree['Outlook'] = {attr[0]:\"\"}\n",
        "tree['Outlook'].update({attr[1]:\"\"})\n",
        "tree['Outlook'].update({attr[2]:\"\"})\n",
        "\n",
        "\n",
        "print(tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Outlook', 'Temp', 'Humidity', 'Windy', 'Play'], dtype='object')\n",
            "['sunny' 'overcast' 'rainy']\n",
            "{'Outlook': {'sunny': '', 'overcast': '', 'rainy': ''}}\n",
            "{'Outlook': {'sunny': 0, 'overcast': '', 'rainy': ''}}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}