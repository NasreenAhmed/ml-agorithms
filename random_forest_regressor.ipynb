{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random_forest_regressor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Contents \n",
        "\n",
        "1. Import \"Hitters\" Data ISLR\n",
        "2. Modified Regression Tree Regressor\n",
        "3. Random Forest Regressor"
      ],
      "metadata": {
        "id": "au1dZAPLmnpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='blue'>1. Import \"hitters\" Data ISLR</font>"
      ],
      "metadata": {
        "id": "QOx-BvF1C17d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSEofnF7HdGa",
        "outputId": "a56b97d8-5c75-4330-afd8-34c2f94e48fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "%cd /content/drive/My\\ Drive/colab_notebooks/machine_learning/data/\n",
        "df = pd.read_csv(\"Hitters.csv\")"
      ],
      "metadata": {
        "id": "dupfaODiHnxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c531b3f-dc6b-4566-8c57-0f27fe17a9a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/colab_notebooks/machine_learning/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "p6ljwJPfH1lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "703c4b04-b1fc-4650-e761-2163b6b16bcf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'Years',\n",
              "       'CAtBat', 'CHits', 'CHmRun', 'CRuns', 'CRBI', 'CWalks', 'League',\n",
              "       'Division', 'PutOuts', 'Assists', 'Errors', 'Salary', 'NewLeague'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'Years',\n",
        "       'CAtBat', 'CHits', 'CHmRun', 'CRuns', 'CRBI', 'CWalks'\n",
        "       , 'PutOuts', 'Assists', 'Errors', 'Salary']]\n",
        "df = df.dropna()\n",
        "df.columns"
      ],
      "metadata": {
        "id": "TZYm5lq4Hsk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372f3498-07c6-4b81-adff-3d339c678a27"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'Years', 'CAtBat',\n",
              "       'CHits', 'CHmRun', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists',\n",
              "       'Errors', 'Salary'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df['Salary']= np.log(df['Salary'])\n",
        "df =  (df - np.mean(df,axis=0))/np.std(df,axis=0)\n",
        "sub_df = df.sample(frac=1)\n",
        "train_df = sub_df.iloc[0:210,:]\n",
        "test_df = sub_df.iloc[210:,:]\n",
        "# Array implentation\n",
        "train_arr = np.array(train_df)\n",
        "col = train_df.columns"
      ],
      "metadata": {
        "id": "dyNlcJezMg9k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='blue'>2. Modified Regression Tree</font>\n",
        "\n",
        "Our earlier implementation of regression tree,was a single tree implementation without the constraint of MaxDepth. \n",
        "\n",
        "But when we are using our regression tree for building a random forest, we need to update our tree in two aspects. \n",
        "1. To add a constraint of max depth- This is not a part of random forest algorithm, but since we are bagging(aggregating) the prediction from multiple trees, we can save on some execution time and also not overfitting the data when it comes to execution of individual instances of trees.\n",
        "2. As per the requirement of the random forest algorithm, rather than having all the features available at very node, we only have a random number of features $m$ out of the total features $p$, such that $m\\leq p$. \n",
        "As per ESL total $m$ for regression tree should ideally be equal to $m=\\frac{p}{3}$.\\\n",
        "  In our example, we have total 16 features, so $\\frac{16}{3} â‰ˆ 5 $ random features per node."
      ],
      "metadata": {
        "id": "F3v3R8OJIaTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################\n",
        "#Node class \n",
        "\n",
        "# Abstract Datatype created, to hold value, and \n",
        "# pointer to subsequent recursive Nodes created\n",
        "##########################################################\n",
        "class Node:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.data = None\n",
        "    self.feature_index = None\n",
        "    self.threshold = None\n",
        "    self.leaf_val = None\n",
        "    self.left = None\n",
        "    self.right = None\n",
        "\n",
        "\n",
        "##########################################################\n",
        "#least Square Error Function to select the best threshold value\n",
        "# per feature.\n",
        "#\n",
        "#Input: Array of single feature & response \n",
        "#Outout: minimum error calculated &threshold value\n",
        "##########################################################\n",
        "\n",
        "def leastsquares_error(arr):\n",
        "\n",
        "  n = len(arr)\n",
        "  #print(\"arr\",len(arr),arr.shape)\n",
        "  #Begin splitting logic\n",
        "  i = 0\n",
        "  min_err = 99999999999\n",
        "  while i < n-1:\n",
        "\n",
        "    split = arr[i,0]\n",
        "    left_split = arr[arr[:,0] <= split]\n",
        "    #print(\"left_split\",left_split)\n",
        "\n",
        "    right_split = arr[arr[:,0] > split]\n",
        "    #print(\"right_split\",right_split)\n",
        "    #print(\"right split\",right_split.shape)\n",
        "\n",
        "    # length of the splits \n",
        "    nl = len(left_split)\n",
        "    nr = len(right_split)\n",
        "    #print(\"nl,nr\",nl,nr)\n",
        "    #print(\"here here- 2\")\n",
        "    kl = np.mean(left_split,axis=0)[1]\n",
        "    kr = np.mean(right_split,axis=0)[1]\n",
        "    \n",
        "    delta_err = np.sum(((left_split[:,-1] - kl)**2)) + np.sum(((right_split[:,-1] - kr)**2))\n",
        "    #print(\"here here- 4\")\n",
        "    if delta_err < min_err:\n",
        "      min_err = delta_err\n",
        "      threshold = split\n",
        "      \n",
        "    i +=1\n",
        "\n",
        "  return min_err, threshold\n",
        "\n",
        "\n",
        "##########################################################\n",
        "#Best Split Function \n",
        "#\n",
        "#Input : dataframe/array consisting of selected features\n",
        "#         with response variable.\n",
        "#Output : Best feature inder, along with the corresponding \n",
        "#         threshold\n",
        "##########################################################\n",
        "\n",
        "def best_split(arr,col):\n",
        "\n",
        "  #print(\"arr\",arr.shape)\n",
        "  #print(\"col\",col)\n",
        "  m = arr.shape[1]\n",
        "  split_err = []\n",
        "  split_threshold = []\n",
        "\n",
        "  #for i in range(0,m-1):\n",
        "  for i in range(0,2):\n",
        "\n",
        "    #print(\"here -1 \")\n",
        "    min_err,threshold = leastsquares_error(arr[:,[i,-1]])\n",
        "    #print(\"here -2\")\n",
        "    split_err.append(min_err)\n",
        " \n",
        "    split_threshold.append(threshold)\n",
        " \n",
        "\n",
        "  best_split_index = np.argmin(split_err)\n",
        "  colname = col[best_split_index]   # adding the colname\n",
        "  best_threshold = split_threshold[best_split_index]\n",
        "\n",
        "  #print(\"best_threshold\",best_threshold)\n",
        "  #print(\"colname\",colname)\n",
        "  return best_threshold,colname\n",
        "\n",
        "##########################################################\n",
        "#Build Tree Function \n",
        "#\n",
        "#Input : The training dataset, abstract node class, list of columns\n",
        "#Output : Return the head node or the root of the tree.\n",
        "##########################################################\n",
        "\n",
        "def build_tree(arr,col,n_node,depth):\n",
        "\n",
        "  # base case\n",
        "  n = len(arr)\n",
        "  n_node.data = arr\n",
        "\n",
        "  if n <= 10 :\n",
        "    mean = np.mean(arr[:,-1],axis=0)\n",
        "    n_node.mean_value = mean\n",
        "    return n_node\n",
        "  elif depth == 0:\n",
        "    mean = np.mean(arr[:,-1],axis=0)\n",
        "    n_node.mean_value = mean\n",
        "    return n_node\n",
        "\n",
        "  \n",
        "  ##########################################################\n",
        "  # recurring tree build \n",
        "  # Change : Select a random list of features\n",
        "  # Input to the best_split function will be a sub-dataframe  consisting only the \n",
        "  # columns of the respective feature indexes chosen.\n",
        "  ##########################################################\n",
        "  # Place holder to randomly sample m features out of total p features.\n",
        "  # Added the requirement of having a randomized set of features for\n",
        "  # each node selection\n",
        "\n",
        "  lst  = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "  rand_col = random.sample(lst,k = 5) #  n = 16/3 = 5\n",
        "  rand_col.append(-1) # adding the response column at the end\n",
        "  rand_arr = arr[:,rand_col] # selecting the data column \n",
        "  rand_col_names = col[rand_col]  \n",
        "  \n",
        "  \n",
        "  # Continue to the rest of the code \n",
        "\n",
        "  best_threshold,colname  = best_split(rand_arr,rand_col_names) #change \n",
        "  best_split_index = np.where(col == colname) [0]\n",
        "  #test\n",
        " \n",
        "  n_node.feature_index = col[best_split_index]\n",
        "  n_node.threshold = best_threshold\n",
        "\n",
        "  # Split into two brances\n",
        "  \n",
        "  #print(\"best_threshold\",best_threshold)\n",
        "  left_arr  = arr[arr[:,best_split_index[0]] <= best_threshold]\n",
        "  right_arr = arr[arr[:,best_split_index[0]] > best_threshold]\n",
        "\n",
        "  # grow tree\n",
        "  left_node = Node()\n",
        "  right_node = Node()\n",
        "  depth = depth - 1  # Updating the depth Hyperparameter\n",
        "  n_node.left = build_tree(left_arr,col,left_node,depth)\n",
        "  n_node.right = build_tree(right_arr,col,right_node,depth)\n",
        "\n",
        "  return n_node\n",
        "\n",
        "##########################################################\n",
        "#Predict \n",
        "#Input : The test dataset, root of the tree, list of columns\n",
        "#Output : Returns the predicted value.\n",
        "##########################################################\n",
        "\n",
        "def predict_tree_val(arr,cols,noden):\n",
        "\n",
        "  head = noden\n",
        "  index = head.feature_index[0]\n",
        "  threshold = head.threshold\n",
        "\n",
        "  value = arr[index]\n",
        "\n",
        "  if value <= threshold:\n",
        "  \n",
        "    if head.left.feature_index is not None:\n",
        "      pred = predict_tree_val(arr,cols,head.left)\n",
        "    else:\n",
        "      return np.mean(head.data[:,-1])\n",
        "  \n",
        "  elif value > threshold:\n",
        "\n",
        "  \n",
        "    if head.right.feature_index is not None:\n",
        "      pred = predict_tree_val(arr,cols,head.right)\n",
        "    else:\n",
        "      return np.mean(head.data[:-1])\n",
        "\n",
        "  return pred\n"
      ],
      "metadata": {
        "id": "TIS0LS0IIZUT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "n_node = Node()\n",
        "maxdepth = 6\n",
        "\n",
        "\n",
        "#leastsquares_error(train_arr[:,[0,-1]],col[[0,-1]],eps)\n",
        "#best_split_index,best_threshold = best_split(rand_arr,rand_col_names)\n",
        "noden = build_tree(train_arr,col,n_node,maxdepth)"
      ],
      "metadata": {
        "id": "ya_9ADUCEw9C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "#Prediction \n",
        "################################################################\n",
        "\n",
        "test_arr = test_df\n",
        "pred_arr= []\n",
        "for i in range(0,len(test_arr)):\n",
        "  entry = test_arr.iloc[i,:]\n",
        "  pred = predict_tree_val(entry,col,noden)\n",
        "  pred_arr.append(pred)\n",
        "\n",
        "\n",
        "y = test_df.iloc[:,-1]\n",
        "\n",
        "# Prediction Accuracy \n",
        "mean_squared_error = 1/len(test_arr)* np.sum((y - pred_arr)**2)\n",
        "\n",
        "print(\"mean_squared_error\",mean_squared_error)\n",
        "print(\"root mean squared error\",np.sqrt(mean_squared_error))"
      ],
      "metadata": {
        "id": "BGXhTH3keRYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3facd785-0c44-4dd3-e951-fa97cda43362"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_squared_error 0.4500362057868485\n",
            "root mean squared error 0.6708473789073403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "\"\"\" Testing the same dataset, with Sklearn library\"\"\"\n",
        "from sklearn.tree import DecisionTreeRegressor \n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "  \n",
        "clf = DecisionTreeRegressor()\n",
        "clf.fit(train_df.iloc[:,0:-2], train_df.iloc[:,-1])\n",
        "\n",
        "predictions = clf.predict(test_df.iloc[:,0:-2])\n",
        "# Prediction Accuracy \n",
        "mean_squared_error = 1/len(test_df)* np.sum((test_df.iloc[:,-1] - predictions)**2)\n",
        "\n",
        "print(\"mean_squared_error\",mean_squared_error)\n",
        "print(\"root mean squared error\",np.sqrt(mean_squared_error))"
      ],
      "metadata": {
        "id": "KJWFF5sIkjWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae22be10-491e-4a64-9dc8-2c3666a74302"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_squared_error 0.5232047107003388\n",
            "root mean squared error 0.7233289090727252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='blue'>3. Random Forest Regressor</font>\n",
        "\n",
        "1. Bootstrapped samples\n",
        "2. Build tree\n",
        "3. Repeat 1 & 2 for no of trees to be created. \n",
        "4. Predict "
      ],
      "metadata": {
        "id": "wYgMQzNnQf6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class randomforest:\n",
        "\n",
        "  def __init__(self,n_trees=50):\n",
        "    self.n_trees = n_trees\n",
        "    self.arr = None\n",
        "    self.col = None\n",
        "    self.max_depth = None\n",
        "    self.trees = None\n",
        "\n",
        "  # Internal method to the class, called from functions within the \n",
        "  # random forest class\n",
        "  def _bootstrap_samples(self,df):\n",
        "\n",
        "    n = len(df)\n",
        "    self.arr = np.array(df.sample(n,replace=True))\n",
        "    return self.arr\n",
        "\n",
        "  def build_forest(self,df,col,max_depth):\n",
        "\n",
        "    i = 0\n",
        "    self.max_depth = max_depth\n",
        "    self.col = col\n",
        "    self.trees = []\n",
        "\n",
        "    while i < self.n_trees :\n",
        "      \n",
        "      nnode = Node()\n",
        "      self.arr = self._bootstrap_samples(df)\n",
        "      node = build_tree(self.arr,self.col,nnode,self.max_depth)\n",
        "      self.trees.append(node)\n",
        "\n",
        "      i+=1\n",
        "    \n",
        "    return self.trees\n",
        "\n",
        "  # To-Do\n",
        "  #def out_of_bag-accuracy\n",
        "\n",
        "  def predict(self,test_df):\n",
        "\n",
        "    m = len(test_df)\n",
        "    pred_array = np.zeros([test_df.shape[0],len(self.trees)])\n",
        "\n",
        "    for i in range(0,m):\n",
        "      sel_row = test_arr.iloc[i,:]\n",
        "\n",
        "      for j in range(0,len(self.trees)):\n",
        "        pred_array[i,j] = predict_tree_val(sel_row,self.col,self.trees[j])\n",
        "        \n",
        "    predictions = np.mean(pred_array,axis = 1)\n",
        "\n",
        "    #Mean Square Error \n",
        "    mean_squared_error = (1/m) * np.sum((test_df.iloc[:,-1] - predictions)**2)\n",
        "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
        "\n",
        "    return mean_squared_error, root_mean_squared_error\n"
      ],
      "metadata": {
        "id": "8r0NYLTYQfVa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Execution\n",
        "rm = randomforest()\n",
        "max_depth = 6\n",
        "tree = rm.build_forest(train_df,col,max_depth)\n",
        "mean_squared_error,root_mean_squared_error = rm.predict(test_df)\n",
        "\n",
        "print(\"Mean Squared Error of the Random Forest Regressor:\",mean_squared_error)\n",
        "print(\"Root Mean Squared Error of the Random Forest Regressor:\",root_mean_squared_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqng3pGiRrCh",
        "outputId": "efa78afc-afeb-4dac-cace-570efbbc14a7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of the Random Forest Regressor: 0.46859222603494705\n",
            "Root Mean Squared Error of the Random Forest Regressor: 0.6845379653714957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# library \n",
        "# Fitting Random Forest Regression to the dataset\n",
        "# import the regressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        " # create regressor object\n",
        "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        " \n",
        "# fit the regressor with x and y data\n",
        "regressor.fit(train_df.iloc[:,0:-2], train_df.iloc[:,-1]) \n",
        "\n",
        "Y_pred = regressor.predict(test_df.iloc[:,0:-2])\n",
        "\n",
        "mean_squared_error = mean_squared_error(test_df.iloc[:,-1], Y_pred)\n",
        "print(\"Mean Squared Error of the Random Forest Regressor:\",mean_squared_error)\n",
        "print(\"Root Mean Squared Error of the Random Forest Regressor:\",np.sqrt(mean_squared_error))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKZ2YNIKfGcb",
        "outputId": "ce28e8a6-7d42-4da4-a083-882f0c6ad1bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of the Random Forest Regressor: 0.34570706027036124\n",
            "Root Mean Squared Error of the Random Forest Regressor: 0.5879685878262216\n"
          ]
        }
      ]
    }
  ]
}