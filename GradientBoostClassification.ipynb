{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientBoostClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Naive Implementation of Gradient Boosting Classifier\n",
        "\n",
        "1. Transform, split the imported dataset.\n",
        "2. ModelFit, encompassing the logic of gradient boost classifier model\n",
        "3. Predict\n",
        "4. Accuracy of the model\n",
        "\n",
        "Sources: StatQuest\n"
      ],
      "metadata": {
        "id": "uhIwxbQNF3Li"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Kq7yIRjF02J"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "cancer = load_breast_cancer()\n",
        "df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n",
        "df['target'] = cancer['target']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_transform(df):\n",
        "\n",
        "  # Splitting data into training and testing data\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  no_of_train_rows = int(np.floor(0.80*df.shape[0]))\n",
        "  train_df = df.iloc[0:no_of_train_rows,:]\n",
        "  test_df = df.iloc[no_of_train_rows:,:]\n",
        "\n",
        "  l = df.shape[1]\n",
        "  train_x = train_df.iloc[:,0:l-2]\n",
        "  train_y = train_df.iloc[:,-1]\n",
        "  test_x = test_df.iloc[:,0:l-2].reset_index(drop=True)\n",
        "  test_y = test_df.iloc[:,-1].reset_index(drop=True)\n",
        "\n",
        "  return train_x,train_y,test_x,test_y"
      ],
      "metadata": {
        "id": "PiDPpGlmp86t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modelfit(x,y,NofTrees=25,maxLeaf=16,maxDepth=4):\n",
        "\n",
        "  ##################################################################\n",
        "  # Create 4 arrays named Prob,residuals, leaf & leaf_transformed_value\n",
        "  # 1. Prob : holds the probability value for each instance & tree\n",
        "  # 2. residuals : holds the residuals for each instance & tree\n",
        "  # 3. leaf : gives the leaf node values for each instance\n",
        "  # 4. lead_transformed_value = holds the transformed leaf node value\n",
        "  # for each leaf.\n",
        "  ##################################################################\n",
        "  prob = np.zeros((len(y),NofTrees+2))\n",
        "  residuals = np.zeros((len(y),NofTrees+2))\n",
        "  leaf = np.zeros(((maxLeaf),NofTrees+1))\n",
        "  leaf_transformed_value = np.zeros(((maxLeaf),NofTrees+1))\n",
        "\n",
        "  trees = []\n",
        "\n",
        "   ##################################################################\n",
        "  # Calculate the log of Odds, to get the pilot- 0th Probability\n",
        "  # np.log(p,q)\n",
        "  ##################################################################\n",
        "\n",
        "  true_flag = len(y[y == 1].index)\n",
        "  false_flag = len(y[y == 0].index)\n",
        "  prob1 = np.log(true_flag/false_flag) # Pilot Probability\n",
        "  prob[:,0]=np.repeat(prob1,len(y))\n",
        "  residuals[:,0] = y - prob1\n",
        "\n",
        "\n",
        "  for i in range(0,NofTrees+1):\n",
        "\n",
        "  ##################################################################\n",
        "  # Using DecisionTreeRegressor of the Sklearn Library\n",
        "  # Saving the model for prediction of unknown instances.\n",
        "  # tree.apply : Gives the leaf nodes of the tree\n",
        "  ##################################################################\n",
        "\n",
        "    dt = DecisionTreeRegressor(max_depth=maxDepth,max_leaf_nodes=maxLeaf)\n",
        "    node = dt.fit(x, residuals[:,i])\n",
        "    trees.append(node)\n",
        "\n",
        "    leaf_indices=dt.apply(x)\n",
        "    unique_leaves = np.unique(leaf_indices)\n",
        "    leaf[0:len(unique_leaves),i] = unique_leaves\n",
        "  \n",
        "    n_leaf = len(unique_leaves)\n",
        "\n",
        "    for ileaf in range(n_leaf):\n",
        "\n",
        "    ##################################################################\n",
        "    # For each unique leaf index, we aggregate the train_x\n",
        "    # instances, and collect them in local variables within the loop\n",
        "    # 1. res( for residuals corresponding to the unqiue leaf node)\n",
        "    # 2. pb ( for probability corresponding to the unique lead node)\n",
        "    ##################################################################\n",
        "      \n",
        "      leaf_index=unique_leaves[ileaf]\n",
        "      indexes = np.argwhere(leaf_indices == unique_leaves[ileaf])\n",
        "      res = residuals[[indexes],i]    \n",
        "      pb = prob[[indexes],i]\n",
        "\n",
        "      ##################################################################\n",
        "      # 1.calculating the transformed leaf value\n",
        "      # 2.Assigning the transformed value to the corresponding leaf value\n",
        "      #   index in the leaf_transformed_value array, which is directly\n",
        "      #   accessed while predicting new instances.\n",
        "      # Note: One can say storing the leaf values corresponding to leaf nodes\n",
        "      # but only transformed.\n",
        "      ##################################################################\n",
        "      \n",
        "      numerator = np.sum(res)\n",
        "      denomenator = np.sum(pb*(1-pb))\n",
        "      transformed_leaf = numerator/denomenator\n",
        "      leaf_transformed_value[ileaf,i] = transformed_leaf\n",
        "      intermediate_prob = prob[[indexes],0] + 0.1 * transformed_leaf\n",
        "\n",
        "      ##################################################################\n",
        "      # Converting the leaf val to prob, using sigmoid function\n",
        "      ##################################################################\n",
        "      prob[[indexes],i+1] = np.exp(intermediate_prob)/(1+np.exp(intermediate_prob))\n",
        "     \n",
        "      y_temp = np.zeros(len(indexes))\n",
        "      for k in range(0,len(indexes)):\n",
        "        y_temp[k] = y[indexes[k]]\n",
        "\n",
        "      ##################################################################\n",
        "      # Recalculating the new residuals, based on the new calculated\n",
        "      # probability.\n",
        "      ##################################################################\n",
        "      f= y_temp.ravel() - prob[[indexes],i+1].ravel()\n",
        "      residuals[[indexes],i+1] = f.reshape(1,len(indexes),1)\n",
        " \n",
        "  return trees, prob,residuals, leaf, leaf_transformed_value"
      ],
      "metadata": {
        "id": "izvQJkPwR1Oz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x,y,trees,leaf,trans_leaf,lr):\n",
        "\n",
        "  predict_leaf = np.zeros((len(x),len(trees)))\n",
        "  ##################################################################\n",
        "  # Using the Apply functionality of DecisionRegression Tree\n",
        "  # We first extract the leaf terminal where each test instance\n",
        "  # ends and the same is done for all the fitted trees.\n",
        "  ##################################################################\n",
        "  for k in range(len(trees)):\n",
        "    predict_leaf[:,k] = trees[k].apply(x)\n",
        "\n",
        "  ##################################################################\n",
        "  # Formula for Gradient based additive prediction is executed\n",
        "  # in the below lines of code\n",
        "  # pred = pred_pilot + lr(learning_rate)* transformed_leaf_val+...\n",
        "  ##################################################################  \n",
        "  final_pred = []\n",
        "\n",
        "  for i in range(len(x)):\n",
        "    prediction = prob[0,0] \n",
        "    for k in range(len(trees)):\n",
        "\n",
        "      loc = np.argwhere(leaf[:,k] == predict_leaf[i,k])\n",
        "      val = trans_leaf[loc,k]\n",
        "      prediction += lr * val\n",
        "\n",
        "    final_pred.extend(prediction.ravel())\n",
        "\n",
        "  ##################################################################\n",
        "  # 1. Calculating the final predicted probability by transforming\n",
        "  #    predicted value using sigmoid transform.\n",
        "  # 2. Keeping the threshold value @0.5, predictions <=0.5 are rounded\n",
        "  #    of to 0, and predictions > 0.5 are rounded to 1.\n",
        "  ##################################################################\n",
        "  final_prob = np.zeros((len(x),1))\n",
        "  final_prob = np.exp(final_pred)/(1+np.exp(final_pred))\n",
        "\n",
        "  y_pred = np.zeros((len(x),1))\n",
        "  for i in range(len(y)):\n",
        "    if final_prob[i] <= 0.5:\n",
        "      y_pred[i] = 0\n",
        "    else:\n",
        "      y_pred[i] = 1\n",
        "\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "yaN8ryddogzB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################\n",
        "# Function to measure the accuracy of the model\n",
        "##################################################################\n",
        "def accuracy(test_y,y_pred):\n",
        "  count = 0\n",
        "  for i in range(len(test_y)):\n",
        "    if y_pred[i] == test_y[i]:\n",
        "      count +=1\n",
        "  return count/len(test_y) * 100"
      ],
      "metadata": {
        "id": "apbUWMOWpgp1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Execution #\n",
        "\n",
        "train_x,train_y,test_x,test_y = data_transform(df)\n",
        "trees, prob,residuals,leaf, trans_leaf = modelfit(train_x,train_y)\n",
        "lr = 0.1\n",
        "y_pred = predict(test_x,test_y,trees,leaf,trans_leaf,lr)\n",
        "model_accuracy = accuracy(test_y,y_pred)\n",
        "print(\"Accuracy of the Gradient Boost Classification Model:\",model_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdZSdJ30kW99",
        "outputId": "a906912b-1458-48ad-f43f-aac2b3f71122"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Gradient Boost Classification Model: 93.85964912280701\n"
          ]
        }
      ]
    }
  ]
}